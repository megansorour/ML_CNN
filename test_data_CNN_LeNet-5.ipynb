{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "source": [
    "# Machine Learning Project Task 1:\n",
    "Implementation of LeNet-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D, BatchNormalization, AveragePooling2D\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "from keras import regularizers\n",
    "from keras.datasets import mnist \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "b3c56055d1ba56d28d982f9647c33439c46753ff"
   },
   "outputs": [],
   "source": [
    "# Load MNIST dataset from keras\n",
    "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n",
    "rows, cols = 28, 28\n",
    "\n",
    "# Reshape the data into a 4D Array\n",
    "X_train = X_train.reshape(X_train.shape[0], rows, cols, 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], rows, cols, 1)\n",
    "\n",
    "input_shape = (rows,cols,1) \n",
    "\n",
    "# Set type as float32 and normalize the values to [0,1]\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train = X_train / 255.0\n",
    "X_test = X_test / 255.0\n",
    "\n",
    "# Transform labels to one hot encoding\n",
    "Y_train = to_categorical(Y_train, 10)\n",
    "Y_test = to_categorical(Y_test, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "b95ca2c1e71cb5457684eff3c35bb8d68b4a0f97"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Preview images from MNIST dataset\n",
    "plt.figure(figsize=(15,4.5))\n",
    "for i in range(30):  \n",
    "    plt.subplot(3, 10, i+1)\n",
    "    plt.imshow(X_train[i].reshape((28,28)),cmap=plt.cm.binary)\n",
    "    plt.axis('off')\n",
    "plt.subplots_adjust(wspace=-0.1, hspace=-0.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the LeNet-5 CNN model\n",
    "\n",
    "# Define a sequential model\n",
    "model = Sequential()\n",
    "\n",
    "# C1 Convolution Layer\n",
    "model.add(Conv2D(filters=6, strides=(1,1), kernel_size=(5,5), activation='relu', input_shape=(28, 28, 1)))\n",
    "\n",
    "# S2 SubSampling Layer\n",
    "model.add(AveragePooling2D(pool_size=(2,2), strides=(2,2)))\n",
    "\n",
    "# C3 Convolution Layer\n",
    "model.add(Conv2D(filters=6, strides=(1,1), kernel_size=(5,5), activation='relu'))\n",
    "\n",
    "# S4 SubSampling Layer\n",
    "model.add(AveragePooling2D(pool_size=(2,2), strides=(2,2)))\n",
    "\n",
    "# C5 Fully Connected Layer\n",
    "model.add(Dense(units=120, activation='relu'))\n",
    "\n",
    "# Flatten the output so that we can connect it with the fully connected layers by converting it into a 1D Array\n",
    "model.add(Flatten())\n",
    "\n",
    "# FC6 Fully Connected Layers\n",
    "model.add(Dense(units=84, activation='relu'))\n",
    "\n",
    "# Output Layer\n",
    "model.add(Dense(units=10, activation='softmax'))\n",
    "\n",
    "# Compile the Model\n",
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Decrease learning rate each epoch\n",
    "annealer = LearningRateScheduler(lambda x: 1e-3 * 0.95 ** x)\n",
    "\n",
    "# set epochs\n",
    "history = [0] * 1\n",
    "epochs = 25\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "X_train2, X_val2, Y_train2, Y_val2 = train_test_split(X_train, Y_train, test_size=0.1)\n",
    "X_train2, X_test2, Y_train2, Y_test2 = train_test_split(X_train2, Y_train2, test_size=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model using fit method\n",
    "history = model.fit(x=X_train2, y=Y_train2, batch_size=64, epochs=epochs, validation_data=(X_val2, Y_val2), callbacks=[annealer], verbose=2)\n",
    "\n",
    "# Print training and validation accuracy\n",
    "print(\"CNN {0:d}: Epochs={1:d}, Train accuracy={2:.5f}, Validation accuracy={3:.5f}\".format(\n",
    "    1, epochs, max(history.history['accuracy']), max(history.history['val_accuracy'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training and validation accuracy per epoch\n",
    "def show_final_history(history):\n",
    "        fig, ax = plt.subplots(1, 2, figsize=(24,8))\n",
    "        ax[0].set_title('Loss rate over epochs')\n",
    "        ax[0].plot(history.epoch, history.history[\"loss\"], label=\"Training loss\")\n",
    "        ax[0].plot(history.epoch, history.history[\"val_loss\"], label=\"Validation loss\")\n",
    "        ax[0].set_xlabel('Epoch')\n",
    "        ax[0].set_ylabel('Loss (%)')\n",
    "        ax[1].set_title('Accuracy over epochs')\n",
    "        ax[1].plot(history.epoch, history.history[\"accuracy\"], label=\"Training accuracy\")\n",
    "        ax[1].plot(history.epoch, history.history[\"val_accuracy\"], label=\"Validation accuracy\")\n",
    "        ax[1].set_xlabel('Epoch')\n",
    "        ax[1].set_ylabel('Accuracy (%)')\n",
    "        ax[0].legend()\n",
    "        ax[1].legend()\n",
    "\n",
    "show_final_history(history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6e4e01ffe692c34c555bdbf5d606611f9a128b9c"
   },
   "outputs": [],
   "source": [
    "# Use model to make predictions on test data\n",
    "results = np.zeros( (X_test.shape[0],10) ) \n",
    "\n",
    "results = model.predict(X_test)\n",
    "results = np.argmax(results,axis = 1)\n",
    "results = pd.Series(results,name=\"Label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "4a9d7710a3b69c2b48bc1687e5b6a27a7076f40a"
   },
   "outputs": [],
   "source": [
    "# Display predicitions and input data\n",
    "plt.figure(figsize=(15,6))\n",
    "for i in range(40):  \n",
    "    plt.subplot(4, 10, i+1)\n",
    "    plt.imshow(X_test[i].reshape((28,28)),cmap=plt.cm.binary)\n",
    "    plt.title(\"Prediction: %d\" % results[i],y=0.9)\n",
    "    plt.axis('off')\n",
    "plt.subplots_adjust(wspace=0.3, hspace=-0.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confusion Matrix\n",
    "#create array for actual and predicted labels\n",
    "\n",
    "#predictions for validation data\n",
    "results2 = model.predict(X_val2)\n",
    "results2 = np.argmax(results2,axis = 1)\n",
    "results2 = pd.Series(results2,name=\"Label\")\n",
    "\n",
    "#create array of true labels\n",
    "Y_val2_labels = np.argmax(Y_val2, axis=1)\n",
    "\n",
    "#compute confusion matrix\n",
    "cm = confusion_matrix(Y_val2_labels,results2)\n",
    "\n",
    "#plot confusion matrix with seaborn library\n",
    "\n",
    "sns.heatmap(cm, annot=True, fmt='g', cmap='PuRd',\n",
    "            xticklabels=np.arange(10), yticklabels=np.arange(10), vmin=0, vmax=10)\n",
    "plt.xlabel('Predicted value',fontsize=13)\n",
    "plt.ylabel('True value',fontsize=13)\n",
    "plt.title('Confusion Matrix',fontsize=17)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "#Showing images that were incorrectly classified\n",
    "\n",
    "# find number of errors\n",
    "num_errors = 0\n",
    "for i in range(len(Y_val2)):\n",
    "    if (Y_val2_labels[i] != results2[i]):\n",
    "        num_errors += 1\n",
    "\n",
    "num_cols = 5  # columns for subplots\n",
    "num_rows = math.ceil(num_errors / num_cols)  # rows\n",
    "\n",
    "# Create subplots\n",
    "fig, axs = plt.subplots(num_rows, num_cols, figsize=(15, 3*num_rows))\n",
    "\n",
    "# plot erroneous images\n",
    "error_count = 0\n",
    "for i in range(len(Y_val2)):\n",
    "    if (Y_val2_labels[i] != results2[i]):\n",
    "        row = error_count // num_cols\n",
    "        col = error_count % num_cols\n",
    "        axs[row, col].imshow(X_val2[i].reshape((28, 28)), cmap=plt.cm.binary)\n",
    "        axs[row, col].set_title(\"Prediction: %d, Actual: %d\" % (results2[i], Y_val2_labels[i]), y=0.9)\n",
    "        axs[row, col].axis('off')\n",
    "        error_count += 1\n",
    "\n",
    "# remove empty subplots\n",
    "for i in range(error_count, num_cols * num_rows):\n",
    "    row = i // num_cols\n",
    "    col = i % num_cols\n",
    "    axs[row, col].axis('off')\n",
    "\n",
    "print('Number of errors: ', num_errors)\n",
    "print('Accuracy %: ', 1-num_errors/len(results2))\n",
    "print('Error %: ', num_errors/len(results2))\n",
    "plt.subplots_adjust(wspace=0.3, hspace=-0.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confusion Matrix\n",
    "\n",
    "#Predictions for unseen test MNIST data\n",
    "results3 = model.predict(X_test2)\n",
    "results3 = np.argmax(results3,axis = 1)\n",
    "results3 = pd.Series(results3,name=\"Label\")\n",
    "\n",
    "Y_test2_labels = np.argmax(Y_test2, axis=1)\n",
    "\n",
    "#compute confusion matrix\n",
    "cm = confusion_matrix(Y_test2_labels,results3)\n",
    "\n",
    "#plot confusion matrix with seaborn\n",
    "sns.heatmap(cm, annot=True, fmt='g', cmap='PuRd',\n",
    "            xticklabels=np.arange(10), yticklabels=np.arange(10), vmin=0, vmax=4)\n",
    "plt.xlabel('Predicted value',fontsize=13)\n",
    "plt.ylabel('True value',fontsize=13)\n",
    "plt.title('Confusion Matrix',fontsize=17)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating model performance using USPS dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Uploading USPS dataset\n",
    "\n",
    "with h5py.File('usps.h5', 'r') as hf:\n",
    "        train = hf.get('train')\n",
    "        X_train_usps = train.get('data')[:]\n",
    "        Y_train_usps = train.get('target')[:]\n",
    "        test = hf.get('test')\n",
    "        X_test_usps = test.get('data')[:]\n",
    "        Y_test_usps = test.get('target')[:]\n",
    "\n",
    "#Process USPS images to be in correct format for model\n",
    "\n",
    "# Pad images with zeros to resize them to 28x28\n",
    "def pad_images(images):\n",
    "    padded_images = []\n",
    "    for image in images:\n",
    "        # Reshape the image to (16, 16)\n",
    "        image = image.reshape(16, 16)\n",
    "        \n",
    "        # Pad the image to (28, 28) with zeros\n",
    "        padded_image = np.pad(image, ((6, 6), (6, 6)), mode='constant')\n",
    "        padded_image = padded_image.reshape(28, 28, 1)\n",
    "        padded_images.append(padded_image)\n",
    "    return np.array(padded_images)\n",
    "\n",
    "X_train_usps_p = pad_images(X_train_usps)\n",
    "X_test_usps_p = pad_images(X_test_usps)\n",
    "\n",
    "# reshape 4D array\n",
    "X_train_usps_p = X_train_usps_p.reshape(-1, 28, 28, 1)\n",
    "X_test_usps = X_test_usps_p.reshape(-1, 28, 28, 1)\n",
    "\n",
    "# One-hot encode labels\n",
    "enc = OneHotEncoder(sparse=False, categories='auto')\n",
    "Y_train_usps = enc.fit_transform(Y_train_usps.reshape(-1, 1))\n",
    "Y_test_usps = enc.transform(Y_test_usps.reshape(-1, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions for USPS\n",
    "results_usps = np.zeros( (X_test_usps_p.shape[0],10) ) \n",
    "results_usps = model.predict(X_test_usps_p)\n",
    "results_usps = np.argmax(results_usps,axis = 1)\n",
    "results_usps = pd.Series(results_usps,name=\"Label\")\n",
    "\n",
    "# Preview USPS predictions\n",
    "plt.figure(figsize=(15,6))\n",
    "for i in range(40):  \n",
    "    plt.subplot(4, 10, i+1)\n",
    "    plt.imshow(X_train_usps[i].reshape((28,28)),cmap=plt.cm.binary)\n",
    "    plt.title(\"Prediction: %d\" % results_usps[i],y=0.9)\n",
    "    plt.axis('off')\n",
    "plt.subplots_adjust(wspace=0.3, hspace=-0.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calc accuracy on USPS test data\n",
    "Y_test_usps_labels = np.argmax(Y_test_usps, axis=1)\n",
    "error_usps = 0\n",
    "#show erroneous ones\n",
    "for i in range(len(Y_test_usps_labels)):\n",
    "    if (Y_test_usps_labels[i] != results_usps[i]):\n",
    "        error_usps = error_usps + 1\n",
    "\n",
    "print('Number of errors: ', error_usps)\n",
    "print('Total classifications: ', len(Y_test_usps_labels))\n",
    "print('Accuracy %: ', 1-(error_usps/len(Y_test_usps_labels)))\n",
    "print('Error %: ', (error_usps/len(Y_test_usps_labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute confusion matrix\n",
    "cm = confusion_matrix(Y_test_usps_labels, results_usps)\n",
    "\n",
    "#plot confusion matrix with seaborn\n",
    "\n",
    "sns.heatmap(cm, annot=True, fmt='g', cmap='PuRd',\n",
    "            xticklabels=np.arange(10), yticklabels=np.arange(10), vmin=0, vmax=25)\n",
    "plt.xlabel('Predicted value',fontsize=13)\n",
    "plt.ylabel('True value',fontsize=13)\n",
    "plt.title('Confusion Matrix',fontsize=17)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 861823,
     "sourceId": 3004,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 11105,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
